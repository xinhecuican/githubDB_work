{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_dataset(feature_file, label_file):\n",
        "    ''' Read data set in *.csv to data frame in Pandas'''\n",
        "    df_X = pd.read_csv(feature_file)\n",
        "    df_y = pd.read_csv(label_file)\n",
        "    X = df_X.values  # convert values in dataframe to numpy array (features)\n",
        "    y = df_y.values  # convert values in dataframe to numpy array (label)\n",
        "    return X, y\n",
        "\n",
        "def normalize_features(X_train, X_test):\n",
        "    from sklearn.preprocessing import StandardScaler  # import libaray\n",
        "    scaler = StandardScaler()  # call an object function\n",
        "    scaler.fit(X_train)  # calculate mean, std in X_train\n",
        "    X_train_norm = scaler.transform(X_train)  # apply normalization on X_train\n",
        "    # we use the same normalization on X_test\n",
        "    X_test_norm = scaler.transform(X_test)\n",
        "    return X_train_norm, X_test_norm\n",
        "\n",
        "def one_hot_encoder(y_train, y_test):\n",
        "    ''' convert label to a vector under one-hot-code fashion. '''\n",
        "    from sklearn import preprocessing\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(y_train)\n",
        "    y_train_ohe1 = lb.transform(y_train)\n",
        "    y_test_ohe1 = lb.transform(y_test)\n",
        "    y_train_ohe = np.where(y_train_ohe1 > 0, y_train_ohe1, -1) #In SVM, we label negative class as âˆ’1 instead of 0.\n",
        "    y_test_ohe = np.where(y_test_ohe1 > 0, y_test_ohe1, -1) \n",
        "    return y_train_ohe, y_test_ohe\n",
        "\n",
        "def accuracy(ypred, yexact):\n",
        "    p = np.array(ypred == yexact, dtype=int)\n",
        "    return np.sum(p) / float(len(yexact))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Input information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "(1347, 64)\n(450, 64)\n(1347, 10)\n(450, 10)\n"
        }
      ],
      "source": [
        "# X_train, y_train = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
        "# X_test, y_test = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')\n",
        "X_train, y_train = read_dataset('Digits_X_train.csv', 'Digits_y_train.csv')\n",
        "X_test, y_test = read_dataset('Digits_X_test.csv', 'Digits_y_test.csv')\n",
        "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
        "y_train_ohe, y_test_ohe = one_hot_encoder(y_train, y_test)\n",
        "print(X_train_norm.shape)\n",
        "print(X_test_norm.shape)\n",
        "print(y_train_ohe.shape)\n",
        "print(y_test_ohe.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stucture of SVM (Classification version)\n",
        "$N$: #of samples, $D$: #of features\n",
        "\n",
        "Then $X$ has shape $(N,D)$, $W$ has shape $(D,10)$, $y$ has shape $(N,10)$, and we have:\n",
        "\n",
        "\\begin{align}\n",
        "    \\hat{y}  &= XW + b \\\\ \n",
        "    L        &= \\|W\\|_2 +\\lambda \\sum_{i}(0, 1-y^{i}W^Tx^{i}-by^i)  \\\\\n",
        "    \\dfrac{\\partial{L}}{\\partial{W}}  &= \\frac{W}{\\|W\\|_2} - \\lambda (0, (y^T X)^T)\\\\\n",
        "    \\dfrac{\\partial{L}}{\\partial{b}}  &= -\\sum_{i}\\lambda (0, y^i) \\\\\n",
        "    W                                 &:= W - lr*\\dfrac{\\partial{L}}{\\partial{W}}\\\\\n",
        "    b                                 &:= b - lr *\\dfrac{\\partial{L}}{\\partial{b}}\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[7 3 0 2 3 1 2 0 5 6 4 1 7 6 1 5 4 8 3 8 5 9 7 9 5 6 1 7 3 0 2 2 5 2 5 4 1\n 7 2 7 8 3 9 6 3 2 9 7 6 8 8 9 7 1 0 7 2 5 7 0 8 7 6 8 9 7 8 6 8 9 1 6 1 9\n 0 2 1 4 9 9 9 9 6 9 0 1 3 8 8 6 0 6 4 8 1 9 6 8 9 6 6 3 6 6 5 3 1 0 2 8 2\n 2 0 0 7 2 5 6 6 2 5 4 3 2 2 1 7 1 7 4 7 3 6 9 0 2 1 4 9 8 3 6 4 0 7 4 6 4\n 0 5 0 1 2 8 9 2 4 4 5 0 9 6 7 1 1 1 9 2 3 7 9 7 2 3 8 6 3 6 0 1 1 1 9 2 4\n 5 0 3 1 3 7 4 0 2 5 0 7 6 8 4 6 9 5 7 4 4 3 7 1 9 0 6 1 4 1 8 1 4 5 1 2 0\n 2 0 3 0 5 2 0 3 3 6 5 2 9 4 2 6 1 6 0 3 9 2 8 5 2 6 3 6 7 4 6 1 2 1 1 9 5\n 3 3 4 9 7 5 1 9 6 8 9 2 8 7 1 5 7 4 3 6 0 6 5 7 2 4 8 3 3 4 1 4 2 5 9 3 8\n 9 3 5 4 5 6 4 1 6 2 7 9 5 1 3 5 0 1 6 8 5 7 5 3 7 7 4 0 2 3 2 6 3 4 1 0 3\n 4 9 2 9 4 9 1 3 1 7 4 5 3 7 1 6 6 3 9 5 2 9 4 0 0 2 0 1 2 5 8 6 8 5 2 0 9\n 6 4 9 7 8 6 4 9 5 6 6 3 2 0 4 1 9 2 0 9 3 1 2 0 1 4 1 6 3 5 2 1 3 9 7 7 1\n 2 0 3 8 8 7 3 5 0 1 3 7 7 3 1 8 8 0 6 3 3 3 9 3 7 9 7 0 8 6 2 2 1 0 0 7 6\n 3 6 6 6 4 1]\n[7 3 0 2 3 1 2 0 5 6 4 1 7 6 1 5 4 8 3 0 5 9 7 9 5 6 1 7 3 0 2 2 5 2 5 4 1\n 7 2 7 8 3 9 6 3 2 9 7 6 8 1 9 7 1 2 7 2 5 9 0 8 7 6 8 9 7 8 6 8 9 1 6 1 9\n 0 2 1 7 9 9 9 9 6 7 0 1 3 8 8 6 0 6 4 8 1 8 6 8 9 1 6 3 6 6 5 3 1 0 2 8 2\n 2 0 0 7 2 5 6 6 2 5 4 3 2 2 1 7 1 7 4 7 3 6 9 0 2 1 4 9 8 3 9 4 0 7 8 6 0\n 0 5 0 1 2 8 9 2 4 4 5 0 9 6 7 1 4 1 9 2 3 7 9 7 2 3 8 6 3 8 0 1 1 1 9 2 4\n 5 0 3 1 9 7 4 0 2 5 0 8 6 8 4 6 9 5 7 4 4 3 7 1 9 0 5 1 4 1 8 1 4 5 1 2 0\n 2 0 3 0 0 2 0 3 3 6 5 2 9 4 2 6 1 6 0 3 9 2 8 5 2 6 3 6 7 4 6 1 2 1 1 9 5\n 3 3 4 9 7 5 1 9 6 8 9 2 8 7 1 5 7 4 3 6 0 6 5 7 8 4 3 3 3 4 1 4 2 5 9 7 3\n 9 3 5 4 5 6 4 8 6 2 7 9 5 8 3 3 0 1 6 8 9 7 5 3 7 7 4 0 2 3 2 6 3 4 1 0 3\n 4 9 2 9 8 9 1 3 1 7 4 5 3 7 1 6 6 3 9 5 2 9 4 0 0 2 0 1 2 5 8 6 8 5 2 0 9\n 6 4 9 7 8 8 4 9 5 6 6 3 2 0 4 1 9 2 0 9 3 1 2 0 1 4 1 6 3 5 2 1 3 9 7 7 1\n 2 0 3 8 8 7 3 5 0 1 3 7 7 3 1 8 8 0 6 3 3 3 9 3 9 9 7 0 8 6 2 2 1 0 0 7 6\n 3 6 6 6 4 1]\nAccuracy of our model  0.9377777777777778\n"
        }
      ],
      "source": [
        "class SVM():\n",
        "    def __init__(self, X, y, lr=0.01, Lambda=0.01):\n",
        "        '''\n",
        "        Input:\n",
        "        - X: shape (M,N)\n",
        "        - y: shape (M,P) P = 10\n",
        "        - W shape (N,P)\n",
        "        '''\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.M = X.shape[0]  # numbers of samples\n",
        "        self.N = X.shape[1]  # numbers of features\n",
        "        self.P = y.shape[1]\n",
        "        self.W = np.random.randn(self.N, self.P)  # 784 by 10\n",
        "        self.b = np.zeros((1, self.P))   # 1 by 10\n",
        "        self.lr = lr\n",
        "        self.Lambda = Lambda\n",
        "\n",
        "    def forward(self):\n",
        "        self.y_hat = np.dot(self.X, self.W) + self.b # 2000 by 10\n",
        "        self.cond = 1 - self.y * self.y_hat\n",
        "\n",
        "    def loss(self):\n",
        "        self.forward()\n",
        "        self.hinge_loss = np.where(self.cond > 0, self.y, 0)  # 2000 by 10\n",
        "        self.loss = np.linalg.norm(self.W, axis=0) + self.Lambda * self.hinge_loss\n",
        "\n",
        "    def sub_gradient_descent(self):\n",
        "        y = np.where(self.cond > 0, self.y, 0)\n",
        "        y_b = np.where(self.cond > 0, self.y, 0)\n",
        "        dW = (1/np.linalg.norm(self.W, axis=0)) * self.W - \\\n",
        "            self.Lambda * np.dot(y.T, self.X).T\n",
        "        db = -np.sum(self.Lambda * y_b ,axis = 0)\n",
        "        self.W = self.W - self.lr * dW\n",
        "        self.b = self.b - self.lr * db\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        y_hat_test = np.dot(X_test, self.W) + self.b\n",
        "        labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "        num_test_samples = X_test.shape[0]\n",
        "        ypred = np.zeros(num_test_samples, dtype=int)\n",
        "        for i in range(num_test_samples):\n",
        "            ypred[i] = labels[np.argmax(y_hat_test[i, :])]\n",
        "        return ypred\n",
        "\n",
        "mySVM = SVM(X_train_norm, y_train_ohe, lr=0.01, Lambda=200)\n",
        "epoch_num = 2000\n",
        "for i in range(epoch_num):\n",
        "    mySVM.forward()\n",
        "    mySVM.sub_gradient_descent()\n",
        "   \n",
        "y_pred = mySVM.predict(X_test_norm)\n",
        "print(y_pred)\n",
        "print(y_test.ravel())\n",
        "print('Accuracy of our model ', accuracy(y_pred, y_test.ravel()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Accuracy of library model  0.9488888888888889\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC()\n",
        "clf.fit(X_train_norm, y_train.ravel()) \n",
        "y_hat = clf.predict(X_test_norm)\n",
        "print('Accuracy of library model ', accuracy(y_hat, y_test.ravel()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}